AWSTemplateFormatVersion: '2010-09-09'
Description: |
  Streaming ETL Framework - One-Click Deployment
  Creates complete infrastructure: VPC, MSK, RDS, DMS, Glue, and utility Lambdas.
  Automatically sets up database tables, Kafka topics, and starts the pipeline.

Metadata:
  AWS::CloudFormation::Interface:
    ParameterGroups:
      - Label:
          default: "Quick Start Configuration"
        Parameters:
          - EnvironmentName
      - Label:
          default: "Database Settings"
        Parameters:
          - RDSPassword
          - DBInstanceClass
      - Label:
          default: "Kafka Settings"
        Parameters:
          - MSKPassword
          - MSKInstanceType
      - Label:
          default: "Processing Settings"
        Parameters:
          - GlueWorkerType
          - GlueNumberOfWorkers
          - DMSInstanceClass
          - CreateDMSVPCRole
    ParameterLabels:
      EnvironmentName:
        default: "Environment Name (used as prefix for all resources)"
      RDSPassword:
        default: "RDS Database Password"
      MSKPassword:
        default: "Kafka SASL Password"

Parameters:
  EnvironmentName:
    Type: String
    Default: etl-streaming
    Description: Prefix for all resource names
    AllowedPattern: ^[a-z0-9-]+$
    ConstraintDescription: Must be lowercase alphanumeric with hyphens

  RDSPassword:
    Type: String
    Default: Amazon123
    NoEcho: true
    MinLength: 8
    MaxLength: 41
    Description: Password for RDS PostgreSQL (min 8 characters)

  MSKPassword:
    Type: String
    Default: KafkaAdmin1234
    NoEcho: true
    MinLength: 8
    MaxLength: 41
    Description: Password for Kafka SASL/SCRAM authentication (min 8 characters)

  DBInstanceClass:
    Type: String
    Default: db.t3.micro
    AllowedValues:
      - db.t3.micro
      - db.t3.small
      - db.t3.medium
    Description: RDS instance size

  MSKInstanceType:
    Type: String
    Default: kafka.t3.small
    AllowedValues:
      - kafka.t3.small
      - kafka.m5.large
    Description: MSK broker instance size

  DMSInstanceClass:
    Type: String
    Default: dms.t3.small
    AllowedValues:
      - dms.t3.small
      - dms.t3.medium
    Description: DMS replication instance size

  GlueWorkerType:
    Type: String
    Default: G.1X
    AllowedValues:
      - G.1X
      - G.2X
    Description: Glue worker type

  GlueNumberOfWorkers:
    Type: Number
    Default: 2
    MinValue: 2
    MaxValue: 10
    Description: Number of Glue workers

  CreateDMSVPCRole:
    Type: String
    Default: 'true'
    AllowedValues:
      - 'true'
      - 'false'
    Description: Set to false if dms-vpc-role already exists in this account

  DMSTableMappings:
    Type: String
    Default: '{"rules":[]}'
    Description: DMS table mappings JSON (generated by config compiler). Pass via --parameter-overrides during deploy.

Conditions:
  ShouldCreateDMSVPCRole: !Equals [!Ref CreateDMSVPCRole, 'true']

Resources:
  # ==========================================================================
  # VPC and Networking
  # ==========================================================================
  VPC:
    Type: AWS::EC2::VPC
    Properties:
      CidrBlock: 10.0.0.0/16
      EnableDnsHostnames: true
      EnableDnsSupport: true
      Tags:
        - Key: Name
          Value: !Sub '${EnvironmentName}-vpc'

  InternetGateway:
    Type: AWS::EC2::InternetGateway

  InternetGatewayAttachment:
    Type: AWS::EC2::VPCGatewayAttachment
    Properties:
      InternetGatewayId: !Ref InternetGateway
      VpcId: !Ref VPC

  PrivateSubnet1:
    Type: AWS::EC2::Subnet
    Properties:
      VpcId: !Ref VPC
      AvailabilityZone: !Select [0, !GetAZs '']
      CidrBlock: 10.0.1.0/24
      Tags:
        - Key: Name
          Value: !Sub '${EnvironmentName}-private-1'

  PrivateSubnet2:
    Type: AWS::EC2::Subnet
    Properties:
      VpcId: !Ref VPC
      AvailabilityZone: !Select [1, !GetAZs '']
      CidrBlock: 10.0.2.0/24
      Tags:
        - Key: Name
          Value: !Sub '${EnvironmentName}-private-2'

  PublicSubnet1:
    Type: AWS::EC2::Subnet
    Properties:
      VpcId: !Ref VPC
      AvailabilityZone: !Select [0, !GetAZs '']
      CidrBlock: 10.0.101.0/24
      MapPublicIpOnLaunch: true
      Tags:
        - Key: Name
          Value: !Sub '${EnvironmentName}-public-1'

  NatGatewayEIP:
    Type: AWS::EC2::EIP
    DependsOn: InternetGatewayAttachment
    Properties:
      Domain: vpc

  NatGateway:
    Type: AWS::EC2::NatGateway
    Properties:
      AllocationId: !GetAtt NatGatewayEIP.AllocationId
      SubnetId: !Ref PublicSubnet1

  PublicRouteTable:
    Type: AWS::EC2::RouteTable
    Properties:
      VpcId: !Ref VPC

  DefaultPublicRoute:
    Type: AWS::EC2::Route
    DependsOn: InternetGatewayAttachment
    Properties:
      RouteTableId: !Ref PublicRouteTable
      DestinationCidrBlock: 0.0.0.0/0
      GatewayId: !Ref InternetGateway

  PublicSubnet1RouteTableAssociation:
    Type: AWS::EC2::SubnetRouteTableAssociation
    Properties:
      RouteTableId: !Ref PublicRouteTable
      SubnetId: !Ref PublicSubnet1

  PrivateRouteTable:
    Type: AWS::EC2::RouteTable
    Properties:
      VpcId: !Ref VPC

  DefaultPrivateRoute:
    Type: AWS::EC2::Route
    Properties:
      RouteTableId: !Ref PrivateRouteTable
      DestinationCidrBlock: 0.0.0.0/0
      NatGatewayId: !Ref NatGateway

  PrivateSubnet1RouteTableAssociation:
    Type: AWS::EC2::SubnetRouteTableAssociation
    Properties:
      RouteTableId: !Ref PrivateRouteTable
      SubnetId: !Ref PrivateSubnet1

  PrivateSubnet2RouteTableAssociation:
    Type: AWS::EC2::SubnetRouteTableAssociation
    Properties:
      RouteTableId: !Ref PrivateRouteTable
      SubnetId: !Ref PrivateSubnet2

  # ==========================================================================
  # Security Groups
  # ==========================================================================
  MSKSecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: MSK cluster security group
      VpcId: !Ref VPC
      SecurityGroupIngress:
        - IpProtocol: tcp
          FromPort: 9096
          ToPort: 9096
          CidrIp: 10.0.0.0/16
          Description: SASL/SCRAM from VPC
      Tags:
        - Key: Name
          Value: !Sub '${EnvironmentName}-msk-sg'

  MSKSecurityGroupSelfIngress:
    Type: AWS::EC2::SecurityGroupIngress
    Properties:
      GroupId: !Ref MSKSecurityGroup
      IpProtocol: '-1'
      SourceSecurityGroupId: !Ref MSKSecurityGroup

  RDSSecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: RDS PostgreSQL security group
      VpcId: !Ref VPC
      SecurityGroupIngress:
        - IpProtocol: tcp
          FromPort: 5432
          ToPort: 5432
          CidrIp: 10.0.0.0/16
          Description: PostgreSQL from VPC
      Tags:
        - Key: Name
          Value: !Sub '${EnvironmentName}-rds-sg'

  GlueSecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: Glue jobs security group
      VpcId: !Ref VPC
      Tags:
        - Key: Name
          Value: !Sub '${EnvironmentName}-glue-sg'

  GlueSecurityGroupSelfIngress:
    Type: AWS::EC2::SecurityGroupIngress
    Properties:
      GroupId: !Ref GlueSecurityGroup
      IpProtocol: '-1'
      SourceSecurityGroupId: !Ref GlueSecurityGroup

  LambdaSecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: Lambda functions security group
      VpcId: !Ref VPC
      Tags:
        - Key: Name
          Value: !Sub '${EnvironmentName}-lambda-sg'

  DMSSecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: DMS replication instance security group
      VpcId: !Ref VPC
      Tags:
        - Key: Name
          Value: !Sub '${EnvironmentName}-dms-sg'

  # Allow DMS to connect to MSK on port 9096 for SASL/SCRAM
  MSKSecurityGroupDMSIngress:
    Type: AWS::EC2::SecurityGroupIngress
    Properties:
      GroupId: !Ref MSKSecurityGroup
      IpProtocol: tcp
      FromPort: 9096
      ToPort: 9096
      SourceSecurityGroupId: !Ref DMSSecurityGroup
      Description: Allow DMS to connect to MSK for CDC streaming

  # Allow DMS to connect to RDS on port 5432
  RDSSecurityGroupDMSIngress:
    Type: AWS::EC2::SecurityGroupIngress
    Properties:
      GroupId: !Ref RDSSecurityGroup
      IpProtocol: tcp
      FromPort: 5432
      ToPort: 5432
      SourceSecurityGroupId: !Ref DMSSecurityGroup
      Description: Allow DMS to connect to RDS for CDC source

  # ==========================================================================
  # KMS Key for MSK SCRAM Secret
  # ==========================================================================
  MSKSecretKMSKey:
    Type: AWS::KMS::Key
    Properties:
      Description: KMS key for MSK SCRAM secret
      EnableKeyRotation: true
      KeyPolicy:
        Version: '2012-10-17'
        Statement:
          - Sid: AllowRootAccess
            Effect: Allow
            Principal:
              AWS: !Sub 'arn:aws:iam::${AWS::AccountId}:root'
            Action: 'kms:*'
            Resource: '*'
          - Sid: AllowMSKAccess
            Effect: Allow
            Principal:
              Service: kafka.amazonaws.com
            Action:
              - kms:Decrypt
              - kms:GenerateDataKey
            Resource: '*'

  # ==========================================================================
  # Secrets Manager
  # ==========================================================================
  RDSSecret:
    Type: AWS::SecretsManager::Secret
    Properties:
      Name: !Sub '${EnvironmentName}/rds/credentials'
      SecretString: !Sub '{"username":"postgres","password":"${RDSPassword}"}'

  MSKSecret:
    Type: AWS::SecretsManager::Secret
    Properties:
      Name: !Sub 'AmazonMSK_${EnvironmentName}_scram'
      KmsKeyId: !Ref MSKSecretKMSKey
      SecretString: !Sub '{"username":"kafkaadmin","password":"${MSKPassword}"}'

  # ==========================================================================
  # S3 Buckets
  # ==========================================================================
  DeltaBucket:
    Type: AWS::S3::Bucket
    Properties:
      BucketName: !Sub '${EnvironmentName}-delta-${AWS::AccountId}'
      PublicAccessBlockConfiguration:
        BlockPublicAcls: true
        BlockPublicPolicy: true
        IgnorePublicAcls: true
        RestrictPublicBuckets: true

  AssetsBucket:
    Type: AWS::S3::Bucket
    Properties:
      BucketName: !Sub '${EnvironmentName}-assets-${AWS::AccountId}'
      PublicAccessBlockConfiguration:
        BlockPublicAcls: true
        BlockPublicPolicy: true
        IgnorePublicAcls: true
        RestrictPublicBuckets: true

  QuarantineBucket:
    Type: AWS::S3::Bucket
    Properties:
      BucketName: !Sub '${EnvironmentName}-quarantine-${AWS::AccountId}'
      PublicAccessBlockConfiguration:
        BlockPublicAcls: true
        BlockPublicPolicy: true
        IgnorePublicAcls: true
        RestrictPublicBuckets: true

  # ==========================================================================
  # RDS PostgreSQL
  # ==========================================================================
  DBSubnetGroup:
    Type: AWS::RDS::DBSubnetGroup
    Properties:
      DBSubnetGroupDescription: Subnet group for RDS
      SubnetIds:
        - !Ref PrivateSubnet1
        - !Ref PrivateSubnet2

  DBParameterGroup:
    Type: AWS::RDS::DBParameterGroup
    Properties:
      Family: postgres16
      Description: Enable logical replication for CDC
      Parameters:
        rds.logical_replication: '1'
        max_replication_slots: '10'
        max_wal_senders: '10'

  RDSInstance:
    Type: AWS::RDS::DBInstance
    Properties:
      DBInstanceIdentifier: !Sub '${EnvironmentName}-postgres'
      DBInstanceClass: !Ref DBInstanceClass
      Engine: postgres
      EngineVersion: '16.11'
      DBName: etldb
      MasterUsername: postgres
      MasterUserPassword: !Ref RDSPassword
      AllocatedStorage: 50
      MaxAllocatedStorage: 100
      StorageType: gp3
      DBSubnetGroupName: !Ref DBSubnetGroup
      DBParameterGroupName: !Ref DBParameterGroup
      VPCSecurityGroups:
        - !Ref RDSSecurityGroup
      PubliclyAccessible: false
      MultiAZ: false
      BackupRetentionPeriod: 1
      DeletionProtection: false

  # ==========================================================================
  # MSK Cluster
  # ==========================================================================
  MSKConfiguration:
    Type: AWS::MSK::Configuration
    Properties:
      Name: !Sub '${EnvironmentName}-msk-config'
      ServerProperties: |
        auto.create.topics.enable=true
        delete.topic.enable=true
        default.replication.factor=2
        min.insync.replicas=1
        num.partitions=3
        log.retention.hours=24

  MSKCluster:
    Type: AWS::MSK::Cluster
    DependsOn: RDSInstance
    Properties:
      ClusterName: !Sub '${EnvironmentName}-msk'
      KafkaVersion: '3.5.1'
      NumberOfBrokerNodes: 2
      BrokerNodeGroupInfo:
        InstanceType: !Ref MSKInstanceType
        ClientSubnets:
          - !Ref PrivateSubnet1
          - !Ref PrivateSubnet2
        SecurityGroups:
          - !Ref MSKSecurityGroup
        StorageInfo:
          EBSStorageInfo:
            VolumeSize: 50
      ClientAuthentication:
        Sasl:
          Scram:
            Enabled: true
        Unauthenticated:
          Enabled: false
      ConfigurationInfo:
        Arn: !GetAtt MSKConfiguration.Arn
        Revision: 1
      EncryptionInfo:
        EncryptionInTransit:
          ClientBroker: TLS
          InCluster: true

  MSKScramSecretAssociation:
    Type: AWS::MSK::BatchScramSecret
    DependsOn: MSKCluster
    Properties:
      ClusterArn: !Ref MSKCluster
      SecretArnList:
        - !Ref MSKSecret

  # Custom Resource to get MSK Bootstrap Servers (not available via GetAtt)
  MSKBootstrapLambdaRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub '${EnvironmentName}-msk-bootstrap-role'
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: sts:AssumeRole
      Policies:
        - PolicyName: MSKDescribe
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - kafka:GetBootstrapBrokers
                  - kafka:DescribeCluster
                Resource: '*'
        - PolicyName: Logs
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - logs:CreateLogGroup
                  - logs:CreateLogStream
                  - logs:PutLogEvents
                Resource: '*'

  MSKBootstrapFunction:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Sub '${EnvironmentName}-msk-bootstrap'
      Runtime: python3.10
      Handler: index.handler
      Role: !GetAtt MSKBootstrapLambdaRole.Arn
      Timeout: 60
      Code:
        ZipFile: |
          import boto3
          import cfnresponse
          def handler(event, context):
              if event['RequestType'] == 'Delete':
                  cfnresponse.send(event, context, cfnresponse.SUCCESS, {})
                  return
              try:
                  cluster_arn = event['ResourceProperties']['ClusterArn']
                  client = boto3.client('kafka')
                  response = client.get_bootstrap_brokers(ClusterArn=cluster_arn)
                  bootstrap = response.get('BootstrapBrokerStringSaslScram', '')
                  cfnresponse.send(event, context, cfnresponse.SUCCESS, 
                      {'BootstrapServers': bootstrap})
              except Exception as e:
                  cfnresponse.send(event, context, cfnresponse.FAILED, {'Error': str(e)})

  MSKBootstrapServersResource:
    Type: Custom::MSKBootstrap
    DependsOn: 
      - MSKCluster
      - MSKScramSecretAssociation
    Properties:
      ServiceToken: !GetAtt MSKBootstrapFunction.Arn
      ClusterArn: !Ref MSKCluster

  # ==========================================================================
  # IAM Roles
  # ==========================================================================
  GlueRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub '${EnvironmentName}-glue-role'
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: glue.amazonaws.com
            Action: sts:AssumeRole
      Policies:
        - PolicyName: GlueS3Access
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - s3:GetBucketLocation
                  - s3:ListBucket
                Resource:
                  - !GetAtt DeltaBucket.Arn
                  - !GetAtt AssetsBucket.Arn
                  - !GetAtt QuarantineBucket.Arn
              - Effect: Allow
                Action:
                  - s3:GetObject
                  - s3:PutObject
                  - s3:DeleteObject
                Resource:
                  - !Sub '${DeltaBucket.Arn}/*'
                  - !Sub '${AssetsBucket.Arn}/*'
                  - !Sub '${QuarantineBucket.Arn}/*'
        - PolicyName: GlueSecretsAccess
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - secretsmanager:GetSecretValue
                  - secretsmanager:ListSecrets
                Resource: '*'
              - Effect: Allow
                Action:
                  - kms:Decrypt
                  - kms:GenerateDataKey
                Resource: !GetAtt MSKSecretKMSKey.Arn
        - PolicyName: GlueLogsAccess
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - logs:CreateLogGroup
                  - logs:CreateLogStream
                  - logs:PutLogEvents
                Resource: !Sub 'arn:aws:logs:${AWS::Region}:${AWS::AccountId}:log-group:/aws-glue/*'
        - PolicyName: GlueCatalogAccess
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - glue:GetDatabase
                  - glue:GetTable
                  - glue:GetTables
                  - glue:CreateTable
                  - glue:UpdateTable
                  - glue:GetConnection
                Resource:
                  - !Sub 'arn:aws:glue:${AWS::Region}:${AWS::AccountId}:catalog'
                  - !Sub 'arn:aws:glue:${AWS::Region}:${AWS::AccountId}:database/${EnvironmentName}_db'
                  - !Sub 'arn:aws:glue:${AWS::Region}:${AWS::AccountId}:table/${EnvironmentName}_db/*'
                  - !Sub 'arn:aws:glue:${AWS::Region}:${AWS::AccountId}:connection/*'
        - PolicyName: GlueEC2Access
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - ec2:CreateNetworkInterface
                  - ec2:DeleteNetworkInterface
                  - ec2:DescribeNetworkInterfaces
                  - ec2:DescribeSecurityGroups
                  - ec2:DescribeSubnets
                  - ec2:DescribeVpcs
                  - ec2:DescribeVpcEndpoints
                  - ec2:DescribeRouteTables
                  - ec2:DescribeVpcAttribute
                  - ec2:CreateTags
                  - ec2:DeleteTags
                Resource: '*'
        - PolicyName: GlueMSKAccess
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - kafka:ListClusters
                  - kafka:GetBootstrapBrokers
                  - kafka:DescribeCluster
                Resource: '*'

  LambdaRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub '${EnvironmentName}-lambda-role'
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: sts:AssumeRole
      Policies:
        - PolicyName: LambdaVPCAccess
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - ec2:CreateNetworkInterface
                  - ec2:DescribeNetworkInterfaces
                  - ec2:DeleteNetworkInterface
                Resource: '*'
        - PolicyName: LambdaLogsAccess
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - logs:CreateLogGroup
                  - logs:CreateLogStream
                  - logs:PutLogEvents
                Resource: !Sub 'arn:aws:logs:${AWS::Region}:${AWS::AccountId}:log-group:/aws/lambda/${EnvironmentName}-*'
        - PolicyName: LambdaSecretsAccess
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action: secretsmanager:GetSecretValue
                Resource:
                  - !Ref RDSSecret
                  - !Ref MSKSecret
              - Effect: Allow
                Action:
                  - kms:Decrypt
                  - kms:GenerateDataKey
                Resource: !GetAtt MSKSecretKMSKey.Arn
        - PolicyName: LambdaS3Access
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - s3:GetObject
                  - s3:PutObject
                  - s3:ListBucket
                  - s3:GetBucketLocation
                Resource:
                  - !GetAtt AssetsBucket.Arn
                  - !Sub '${AssetsBucket.Arn}/*'
                  - !GetAtt DeltaBucket.Arn
                  - !Sub '${DeltaBucket.Arn}/*'
        - PolicyName: LambdaAthenaAccess
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - athena:StartQueryExecution
                  - athena:GetQueryExecution
                  - athena:GetQueryResults
                Resource: '*'
              - Effect: Allow
                Action:
                  - glue:GetDatabase
                  - glue:GetTable
                  - glue:GetTables
                  - glue:CreateTable
                  - glue:UpdateTable
                Resource:
                  - !Sub 'arn:aws:glue:${AWS::Region}:${AWS::AccountId}:catalog'
                  - !Sub 'arn:aws:glue:${AWS::Region}:${AWS::AccountId}:database/${EnvironmentName}_db'
                  - !Sub 'arn:aws:glue:${AWS::Region}:${AWS::AccountId}:table/${EnvironmentName}_db/*'

  # ==========================================================================
  # DMS
  # ==========================================================================
  
  # DMS VPC Role - Required for DMS to manage VPC resources
  DMSVPCRole:
    Type: AWS::IAM::Role
    Condition: ShouldCreateDMSVPCRole
    Properties:
      RoleName: dms-vpc-role
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: dms.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AmazonDMSVPCManagementRole

  DMSSubnetGroup:
    Type: AWS::DMS::ReplicationSubnetGroup
    Properties:
      ReplicationSubnetGroupIdentifier: !Sub '${EnvironmentName}-dms-subnet-group'
      ReplicationSubnetGroupDescription: DMS subnet group
      SubnetIds:
        - !Ref PrivateSubnet1
        - !Ref PrivateSubnet2

  DMSReplicationInstance:
    Type: AWS::DMS::ReplicationInstance
    DependsOn: RDSInstance
    Properties:
      ReplicationInstanceIdentifier: !Sub '${EnvironmentName}-dms'
      ReplicationInstanceClass: !Ref DMSInstanceClass
      AllocatedStorage: 50
      EngineVersion: '3.5.4'
      MultiAZ: false
      PubliclyAccessible: false
      ReplicationSubnetGroupIdentifier: !Ref DMSSubnetGroup
      VpcSecurityGroupIds:
        - !Ref DMSSecurityGroup

  DMSSourceEndpoint:
    Type: AWS::DMS::Endpoint
    DependsOn: RDSInstance
    Properties:
      EndpointIdentifier: !Sub '${EnvironmentName}-source-postgres'
      EndpointType: source
      EngineName: postgres
      ServerName: !GetAtt RDSInstance.Endpoint.Address
      Port: 5432
      DatabaseName: etldb
      Username: postgres
      Password: !Ref RDSPassword
      SslMode: require

  DMSTargetEndpoint:
    Type: AWS::DMS::Endpoint
    DependsOn: MSKBootstrapServersResource
    Properties:
      EndpointIdentifier: !Sub '${EnvironmentName}-target-msk'
      EndpointType: target
      EngineName: kafka
      KafkaSettings:
        Broker: !GetAtt MSKBootstrapServersResource.BootstrapServers
        Topic: cdc-default
        SecurityProtocol: sasl-ssl
        SaslUserName: kafkaadmin
        SaslPassword: !Ref MSKPassword
        MessageFormat: json
        IncludeNullAndEmpty: true

  DMSReplicationTask:
    Type: AWS::DMS::ReplicationTask
    DependsOn:
      - DMSReplicationInstance
      - DMSSourceEndpoint
      - DMSTargetEndpoint
    Properties:
      ReplicationTaskIdentifier: !Sub '${EnvironmentName}-cdc-task'
      MigrationType: full-load-and-cdc
      ReplicationInstanceArn: !Ref DMSReplicationInstance
      SourceEndpointArn: !Ref DMSSourceEndpoint
      TargetEndpointArn: !Ref DMSTargetEndpoint
      TableMappings: !Ref DMSTableMappings

  # ==========================================================================
  # Glue
  # ==========================================================================
  GlueDatabase:
    Type: AWS::Glue::Database
    Properties:
      CatalogId: !Ref AWS::AccountId
      DatabaseInput:
        Name: !Sub '${EnvironmentName}_db'
        Description: Streaming ETL Delta Lake database

  GlueMSKConnection:
    Type: AWS::Glue::Connection
    DependsOn: MSKBootstrapServersResource
    Properties:
      CatalogId: !Ref AWS::AccountId
      ConnectionInput:
        Name: !Sub '${EnvironmentName}-msk-connection'
        ConnectionType: KAFKA
        ConnectionProperties:
          KAFKA_BOOTSTRAP_SERVERS: !GetAtt MSKBootstrapServersResource.BootstrapServers
          KAFKA_SSL_ENABLED: 'true'
        PhysicalConnectionRequirements:
          AvailabilityZone: !Select [0, !GetAZs '']
          SecurityGroupIdList:
            - !Ref GlueSecurityGroup
          SubnetId: !Ref PrivateSubnet1

  GlueStreamingJob:
    Type: AWS::Glue::Job
    DependsOn: GlueMSKConnection
    Properties:
      Name: !Sub '${EnvironmentName}-streaming-job'
      Description: Streaming ETL with DQ, Transforms, and SCD2
      Role: !GetAtt GlueRole.Arn
      Command:
        Name: gluestreaming
        ScriptLocation: !Sub 's3://${AssetsBucket}/scripts/glue_streaming_job.py'
        PythonVersion: '3'
      DefaultArguments:
        '--job-language': python
        '--enable-glue-datacatalog': 'true'
        '--enable-continuous-cloudwatch-log': 'true'
        '--CONFIG_PATH': !Sub 's3://${AssetsBucket}/config/tables.yaml'
        '--extra-py-files': !Sub 's3://${AssetsBucket}/libs/PyYAML-6.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl,s3://${AssetsBucket}/libs/pydeequ-1.2.0-py3-none-any.whl,s3://${AssetsBucket}/scripts/deequ_analyzer.py'
        '--extra-jars': !Sub 's3://${AssetsBucket}/libs/deequ-2.0.4-spark-3.3.jar'
        '--datalake-formats': delta
        '--conf': 'spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog'
      ExecutionProperty:
        MaxConcurrentRuns: 1
      Connections:
        Connections:
          - !Ref GlueMSKConnection
      GlueVersion: '4.0'
      WorkerType: !Ref GlueWorkerType
      NumberOfWorkers: !Ref GlueNumberOfWorkers

  # ==========================================================================
  # Utility Lambdas
  # ==========================================================================
  
  # SQL Runner Lambda - Execute SQL against RDS PostgreSQL
  # NOTE: psycopg2 layer is added by deploy.sh after stack creation
  SQLRunnerFunction:
    Type: AWS::Lambda::Function
    DependsOn: RDSInstance
    Properties:
      FunctionName: !Sub '${EnvironmentName}-sql-runner'
      Description: SQL runner utility for RDS PostgreSQL (psycopg2 layer added post-deploy)
      Runtime: python3.10
      Handler: index.lambda_handler
      Role: !GetAtt LambdaRole.Arn
      Timeout: 60
      MemorySize: 256
      Code:
        ZipFile: |
          import json
          import os
          import boto3
          
          def lambda_handler(event, context):
              """
              SQL Runner Lambda - executes SQL against RDS PostgreSQL.
              Requires psycopg2 layer to be attached (done by deploy.sh).
              """
              try:
                  import psycopg2
              except ImportError:
                  return {
                      'statusCode': 500,
                      'body': json.dumps({
                          'error': 'psycopg2 not available. Run deploy.sh to attach the layer.'
                      })
                  }
              
              sql = event.get('sql', '')
              if not sql:
                  return {'statusCode': 400, 'body': json.dumps({'error': 'No SQL provided'})}
              
              # Get credentials from Secrets Manager
              secrets = boto3.client('secretsmanager')
              secret_arn = os.environ.get('RDS_SECRET_ARN')
              secret = json.loads(secrets.get_secret_value(SecretId=secret_arn)['SecretString'])
              
              host = os.environ.get('RDS_HOST')
              database = os.environ.get('RDS_DATABASE', 'etldb')
              user = secret.get('username', 'postgres')
              password = secret.get('password')
              
              try:
                  conn = psycopg2.connect(
                      host=host, database=database,
                      user=user, password=password,
                      connect_timeout=10
                  )
                  conn.autocommit = True
                  cur = conn.cursor()
                  cur.execute(sql)
                  
                  if cur.description:
                      columns = [desc[0] for desc in cur.description]
                      rows = cur.fetchall()
                      result = [dict(zip(columns, row)) for row in rows]
                  else:
                      result = {'rowcount': cur.rowcount}
                  
                  cur.close()
                  conn.close()
                  
                  return {'statusCode': 200, 'body': json.dumps(result, default=str)}
              except Exception as e:
                  return {'statusCode': 500, 'body': json.dumps({'error': str(e)})}
      Environment:
        Variables:
          RDS_HOST: !GetAtt RDSInstance.Endpoint.Address
          RDS_DATABASE: etldb
          RDS_USER: postgres
          RDS_SECRET_ARN: !Ref RDSSecret
      VpcConfig:
        SecurityGroupIds:
          - !Ref LambdaSecurityGroup
        SubnetIds:
          - !Ref PrivateSubnet1
          - !Ref PrivateSubnet2

  # Kafka Admin Lambda - Topic management via SASL/SCRAM
  # NOTE: kafka-python layer is added by deploy.sh after stack creation
  KafkaAdminFunction:
    Type: AWS::Lambda::Function
    DependsOn: MSKBootstrapServersResource
    Properties:
      FunctionName: !Sub '${EnvironmentName}-kafka-admin'
      Description: Kafka admin utility for topic management (kafka-python layer added post-deploy)
      Runtime: python3.10
      Handler: index.lambda_handler
      Role: !GetAtt LambdaRole.Arn
      Timeout: 300
      MemorySize: 256
      Code:
        ZipFile: |
          import json
          import os
          import boto3
          import yaml
          
          def lambda_handler(event, context):
              """
              Kafka Admin Lambda - manages Kafka topics.
              Requires kafka-python layer to be attached (done by deploy.sh).
              
              Actions:
                - list: List all topics
                - create: Create a single topic
                - info: Get connection info
                - sync_from_config: Create topics from tables.yaml config
              """
              try:
                  from kafka.admin import KafkaAdminClient, NewTopic
                  from kafka import KafkaConsumer
                  from kafka.errors import TopicAlreadyExistsError
              except ImportError:
                  return {
                      'statusCode': 500,
                      'body': json.dumps({
                          'error': 'kafka-python not available. Run deploy.sh to attach the layer.',
                          'bootstrap_servers': os.environ.get('KAFKA_BOOTSTRAP', '')
                      })
                  }
              
              action = event.get('action', 'list')
              bootstrap = os.environ.get('KAFKA_BOOTSTRAP', '')
              username = os.environ.get('KAFKA_USERNAME', 'kafkaadmin')
              
              # Get password from Secrets Manager
              secrets = boto3.client('secretsmanager')
              secret_arn = os.environ.get('MSK_SECRET_ARN')
              secret = json.loads(secrets.get_secret_value(SecretId=secret_arn)['SecretString'])
              password = secret.get('password')
              
              kafka_config = {
                  'bootstrap_servers': bootstrap,
                  'security_protocol': 'SASL_SSL',
                  'sasl_mechanism': 'SCRAM-SHA-512',
                  'sasl_plain_username': username,
                  'sasl_plain_password': password,
              }
              
              try:
                  if action == 'list':
                      consumer = KafkaConsumer(**kafka_config)
                      topics = list(consumer.topics())
                      consumer.close()
                      return {'statusCode': 200, 'body': json.dumps({'topics': sorted(topics)})}
                  
                  elif action == 'create':
                      topic_name = event.get('topic')
                      partitions = event.get('partitions', 3)
                      replication = event.get('replication_factor', 2)
                      
                      if not topic_name:
                          return {'statusCode': 400, 'body': json.dumps({'error': 'topic name required'})}
                      
                      admin = KafkaAdminClient(**kafka_config)
                      new_topic = NewTopic(name=topic_name, num_partitions=partitions, replication_factor=replication)
                      try:
                          admin.create_topics([new_topic])
                          result = {'created': topic_name}
                      except TopicAlreadyExistsError:
                          result = {'exists': topic_name}
                      admin.close()
                      return {'statusCode': 200, 'body': json.dumps(result)}
                  
                  elif action == 'sync_from_config':
                      # Load config from S3
                      s3 = boto3.client('s3')
                      config_bucket = os.environ.get('CONFIG_BUCKET')
                      config_key = os.environ.get('CONFIG_KEY', 'config/tables.yaml')
                      
                      response = s3.get_object(Bucket=config_bucket, Key=config_key)
                      config = yaml.safe_load(response['Body'].read().decode('utf-8'))
                      
                      admin = KafkaAdminClient(**kafka_config)
                      consumer = KafkaConsumer(**kafka_config)
                      existing_topics = consumer.topics()
                      consumer.close()
                      
                      created = []
                      skipped = []
                      
                      for table in config.get('tables', []):
                          if table.get('internal', False):
                              continue
                          topic_name = table.get('topic')
                          if not topic_name:
                              continue
                          
                          if topic_name in existing_topics:
                              skipped.append(topic_name)
                          else:
                              new_topic = NewTopic(name=topic_name, num_partitions=3, replication_factor=2)
                              try:
                                  admin.create_topics([new_topic])
                                  created.append(topic_name)
                              except TopicAlreadyExistsError:
                                  skipped.append(topic_name)
                      
                      admin.close()
                      return {
                          'statusCode': 200,
                          'body': json.dumps({'created': created, 'skipped': skipped})
                      }
                  
                  elif action == 'info':
                      return {
                          'statusCode': 200,
                          'body': json.dumps({
                              'bootstrap_servers': bootstrap,
                              'username': username
                          })
                      }
                  
                  else:
                      return {'statusCode': 400, 'body': json.dumps({'error': f'Unknown action: {action}'})}
                      
              except Exception as e:
                  return {'statusCode': 500, 'body': json.dumps({'error': str(e)})}
      Environment:
        Variables:
          KAFKA_BOOTSTRAP: !GetAtt MSKBootstrapServersResource.BootstrapServers
          KAFKA_USERNAME: kafkaadmin
          MSK_SECRET_ARN: !Ref MSKSecret
          CONFIG_BUCKET: !Ref AssetsBucket
          CONFIG_KEY: config/tables.yaml
      VpcConfig:
        SecurityGroupIds:
          - !Ref LambdaSecurityGroup
        SubnetIds:
          - !Ref PrivateSubnet1
          - !Ref PrivateSubnet2

  # Athena Table Creator Lambda - Auto-create Athena tables for Delta Lake
  # Reads schema from tables.yaml in S3 - requires PyYAML layer (added by deploy.sh)
  AthenaTableCreatorFunction:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Sub '${EnvironmentName}-athena-table-creator'
      Description: Creates Athena tables for Delta Lake data (PyYAML layer added post-deploy)
      Runtime: python3.10
      Handler: index.lambda_handler
      Role: !GetAtt LambdaRole.Arn
      Timeout: 300
      MemorySize: 256
      Code:
        ZipFile: |
          import boto3
          import json
          import os
          import time
          
          def lambda_handler(event, context):
              """
              Create Athena tables for Delta Lake tables.
              Reads schema from tables.yaml + adds SCD2 columns from Glue.
              Requires PyYAML layer to be attached (done by deploy.sh).
              """
              try:
                  import yaml
              except ImportError:
                  return {
                      'statusCode': 500,
                      'body': json.dumps({
                          'error': 'PyYAML not available. Run deploy.sh to attach the layer.'
                      })
                  }
              
              s3 = boto3.client('s3')
              athena = boto3.client('athena')
              
              action = event.get('action', 'create_all')
              database = os.environ.get('GLUE_DATABASE')
              delta_bucket = os.environ.get('DELTA_BUCKET')
              config_bucket = os.environ.get('CONFIG_BUCKET')
              config_key = os.environ.get('CONFIG_KEY', 'config/tables.yaml')
              output_location = f's3://{config_bucket}/athena-results/'
              
              # SCD2 columns added by Glue streaming job
              SCD2_COLS = [
                  {'name': '_effective_from', 'type': 'timestamp'},
                  {'name': '_effective_to', 'type': 'timestamp'},
                  {'name': '_is_current', 'type': 'boolean'},
                  {'name': '_ingested_at', 'type': 'timestamp'},
                  {'name': '_operation', 'type': 'string'}
              ]
              
              # Type mapping from tables.yaml to Athena/Hive types
              TYPE_MAP = {
                  'integer': 'INT', 'int': 'INT',
                  'string': 'STRING', 'varchar': 'STRING', 'text': 'STRING',
                  'double': 'DOUBLE', 'float': 'DOUBLE', 'decimal': 'DOUBLE',
                  'boolean': 'BOOLEAN', 'bool': 'BOOLEAN',
                  'timestamp': 'TIMESTAMP', 'datetime': 'TIMESTAMP', 'date': 'STRING'
              }
              
              def load_config():
                  resp = s3.get_object(Bucket=config_bucket, Key=config_key)
                  return yaml.safe_load(resp['Body'].read().decode('utf-8'))
              
              def get_athena_type(col_type):
                  return TYPE_MAP.get(col_type.lower(), 'STRING')
              
              def build_columns(table_config):
                  cols = []
                  for col in table_config.get('columns', []):
                      athena_type = get_athena_type(col['type'])
                      cols.append(f"{col['name']} {athena_type}")
                  # Add SCD2 columns
                  for col in SCD2_COLS:
                      athena_type = get_athena_type(col['type'])
                      cols.append(f"{col['name']} {athena_type}")
                  return ', '.join(cols)
              
              def run_query(ddl):
                  try:
                      resp = athena.start_query_execution(
                          QueryString=ddl,
                          QueryExecutionContext={'Database': database},
                          ResultConfiguration={'OutputLocation': output_location}
                      )
                      qid = resp['QueryExecutionId']
                      for _ in range(30):
                          time.sleep(2)
                          r = athena.get_query_execution(QueryExecutionId=qid)
                          state = r['QueryExecution']['Status']['State']
                          if state in ['SUCCEEDED', 'FAILED', 'CANCELLED']:
                              return {'state': state, 'reason': r['QueryExecution']['Status'].get('StateChangeReason', '')}
                      return {'state': 'TIMEOUT'}
                  except Exception as e:
                      return {'state': 'ERROR', 'reason': str(e)}
              
              def create_table(table_config):
                  name = table_config['name']
                  location = f's3://{delta_bucket}/{name}/'
                  
                  # Create Delta Lake external table (IF NOT EXISTS handles duplicates)
                  ddl = f"CREATE EXTERNAL TABLE IF NOT EXISTS `{database}`.`{name}` LOCATION '{location}' TBLPROPERTIES ('table_type' = 'DELTA')"
                  return run_query(ddl)
              
              try:
                  config = load_config()
                  tables = [t for t in config.get('tables', []) if not t.get('internal', False)]
                  
                  if action == 'create_all':
                      results = {'created': [], 'failed': []}
                      for table in tables:
                          result = create_table(table)
                          if result['state'] == 'SUCCEEDED':
                              results['created'].append(table['name'])
                          else:
                              results['failed'].append({'table': table['name'], 'error': result.get('reason', result['state'])})
                      return {'statusCode': 200, 'body': json.dumps(results)}
                  
                  elif action == 'create':
                      table_name = event.get('table')
                      if not table_name:
                          return {'statusCode': 400, 'body': json.dumps({'error': 'table name required'})}
                      table_config = next((t for t in tables if t['name'] == table_name), None)
                      if not table_config:
                          return {'statusCode': 404, 'body': json.dumps({'error': f'table {table_name} not found'})}
                      result = create_table(table_config)
                      return {'statusCode': 200, 'body': json.dumps({'table': table_name, 'result': result})}
                  
                  elif action == 'drop':
                      table_name = event.get('table')
                      if not table_name:
                          return {'statusCode': 400, 'body': json.dumps({'error': 'table name required'})}
                      result = run_query(f'DROP TABLE IF EXISTS {database}.{table_name}')
                      return {'statusCode': 200, 'body': json.dumps({'table': table_name, 'result': result})}
                  
                  elif action == 'list':
                      return {'statusCode': 200, 'body': json.dumps({'tables': [t['name'] for t in tables]})}
                  
                  else:
                      return {'statusCode': 400, 'body': json.dumps({'error': f'Unknown action: {action}'})}
                      
              except Exception as e:
                  return {'statusCode': 500, 'body': json.dumps({'error': str(e)})}
      Environment:
        Variables:
          GLUE_DATABASE: !Sub '${EnvironmentName}_db'
          DELTA_BUCKET: !Ref DeltaBucket
          CONFIG_BUCKET: !Ref AssetsBucket
          CONFIG_KEY: config/tables.yaml

Outputs:
  VpcId:
    Description: VPC ID
    Value: !Ref VPC

  RDSEndpoint:
    Description: RDS PostgreSQL endpoint
    Value: !GetAtt RDSInstance.Endpoint.Address

  MSKBootstrapServers:
    Description: MSK bootstrap servers (SASL/SCRAM)
    Value: !GetAtt MSKBootstrapServersResource.BootstrapServers

  DeltaBucket:
    Description: S3 bucket for Delta Lake tables
    Value: !Ref DeltaBucket

  AssetsBucket:
    Description: S3 bucket for scripts and configs
    Value: !Ref AssetsBucket

  GlueJobName:
    Description: Glue streaming job name
    Value: !Ref GlueStreamingJob

  DMSTaskArn:
    Description: DMS replication task ARN
    Value: !Ref DMSReplicationTask

  SQLRunnerLambda:
    Description: SQL Runner Lambda function name
    Value: !Ref SQLRunnerFunction

  KafkaAdminLambda:
    Description: Kafka Admin Lambda function name
    Value: !Ref KafkaAdminFunction

  AthenaTableCreatorLambda:
    Description: Athena Table Creator Lambda function name
    Value: !Ref AthenaTableCreatorFunction

  NextSteps:
    Description: Post-deployment steps
    Value: |
      1. Upload scripts to S3: aws s3 sync src/ s3://<AssetsBucket>/scripts/
      2. Upload config to S3: aws s3 cp config/tables.yaml s3://<AssetsBucket>/config/
      3. Upload Deequ libs: aws s3 cp libs/ s3://<AssetsBucket>/libs/ --recursive
      4. Create RDS tables: aws lambda invoke --function-name <SQLRunnerLambda> --payload '{"sql":"..."}' out.json
      5. Create Kafka topics: aws lambda invoke --function-name <KafkaAdminLambda> --payload '{"action":"sync_from_config"}' out.json
      6. Start DMS task: aws dms start-replication-task --replication-task-arn <DMSTaskArn> --start-replication-task-type start-replication
      7. Start Glue job: aws glue start-job-run --job-name <GlueJobName>
      8. Create Athena tables: aws lambda invoke --function-name <AthenaTableCreatorLambda> out.json
