"""
Glue Streaming Job - Multi-Topic CDC with DQ, Transforms, and SCD Type 2.

Reads from multiple Kafka topics (one per table), applies:
1. Schema from S3 config
2. DQ rules from S3 config (quarantine failures)
3. Transforms from S3 config
4. SCD Type 2 MERGE to Delta Lake

Architecture:
- One streaming query per table (parallel execution)
- Config-driven: schema, DQ rules, transforms all in tables.yaml
- MERGE with SCD Type 2 (effective dates, is_current flag)
"""

import json
import logging
import os
import re
import sys
from typing import Any, Dict, List

# Set SPARK_VERSION before importing pydeequ (REQUIRED)
os.environ['SPARK_VERSION'] = '3.3'

import boto3
import yaml
from pyspark.sql import DataFrame, SparkSession
from pyspark.sql.functions import col, lit, current_timestamp, from_json, when
from pyspark.sql.window import Window
from pyspark.sql.types import (
    StructType, StructField, StringType, IntegerType,
    LongType, TimestampType, DecimalType, DateType, BooleanType
)

# Import Deequ analyzer (REQUIRED - job will fail if not available)
from deequ_analyzer import DeequAnalyzer, create_deequ_analyzer

# Logging setup
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[logging.StreamHandler(sys.stdout)]
)
logger = logging.getLogger("GlueStreamingJob")

logger.info("Deequ analyzer loaded successfully - enterprise DQ metrics enabled")


def log_dataframe_sample(df: DataFrame, label: str, table_name: str,
                         num_rows: int = 3) -> None:
    """Log a sample of DataFrame rows for debugging."""
    try:
        count = df.count()
        if count == 0:
            logger.info("[%s] %s: 0 rows", table_name, label)
            return
        logger.info("[%s] %s: %d rows, schema: %s",
                    table_name, label, count, df.columns)
        sample_rows = df.limit(num_rows).collect()
        for i, row in enumerate(sample_rows):
            logger.info("[%s] %s row[%d]: %s", table_name, label, i, row.asDict())
    except Exception as exc:
        logger.warning("[%s] Could not log %s: %s", table_name, label, exc)


# ============================================================================
# DQ Registry
# ============================================================================
class DQRegistry:
    """Registry of DQ rule implementations."""

    def __init__(self):
        self._rules = {
            "range": self._range_check,
            "allowed_values": self._allowed_values_check,
            "regex": self._regex_check,
        }

    def apply(self, dataframe: DataFrame, rule_type: str, column: str,
              params: Dict[str, Any], table_name: str, rule_id: str) -> tuple:
        """Apply a DQ rule. Returns (passed_df, failed_df)."""
        if rule_type not in self._rules:
            logger.warning("[%s] Unknown DQ rule type: %s, skipping",
                           table_name, rule_type)
            return dataframe, dataframe.limit(0)
        try:
            logger.info("[%s] Applying DQ rule '%s': type=%s, column=%s, params=%s",
                        table_name, rule_id, rule_type, column, params)
            return self._rules[rule_type](dataframe, column, params)
        except Exception as exc:
            logger.error("[%s] DQ rule %s failed: %s", table_name, rule_type, exc)
            return dataframe, dataframe.limit(0)

    def _range_check(self, dataframe: DataFrame, column: str,
                     params: Dict[str, Any]) -> tuple:
        """Check column value is within min/max range."""
        cond = col(column).isNotNull()
        if "min" in params:
            cond = cond & (col(column) >= params["min"])
        if "max" in params:
            cond = cond & (col(column) <= params["max"])
        return dataframe.filter(cond), dataframe.filter(~cond)

    def _allowed_values_check(self, dataframe: DataFrame, column: str,
                              params: Dict[str, Any]) -> tuple:
        """Check column value is in allowed list."""
        allowed = params.get("values", [])
        if not allowed:
            return dataframe, dataframe.limit(0)
        cond = col(column).isin(allowed)
        return dataframe.filter(cond), dataframe.filter(~cond)

    def _regex_check(self, dataframe: DataFrame, column: str,
                     params: Dict[str, Any]) -> tuple:
        """Check column value matches regex pattern."""
        pattern = params.get("pattern", ".*")
        cond = col(column).rlike(pattern)
        return dataframe.filter(cond), dataframe.filter(~cond)



# ============================================================================
# Transform Registry
# ============================================================================
class TransformRegistry:
    """Registry of transform implementations."""

    def __init__(self):
        from pyspark.sql.functions import trim, lower, upper, round as spark_round
        from pyspark.sql.functions import concat, substring

        self._trim = trim
        self._lower = lower
        self._upper = upper
        self._round = spark_round
        self._concat = concat
        self._substring = substring

        self._transforms = {
            "trim": self._trim_transform,
            "lower": self._lower_transform,
            "upper": self._upper_transform,
            "round": self._round_transform,
            "mask_pii": self._mask_pii_transform,
        }

    def apply(self, dataframe: DataFrame, transform_type: str, column: str,
              params: Dict[str, Any], table_name: str, transform_id: str) -> DataFrame:
        """Apply a transform. Returns modified DataFrame."""
        if transform_type not in self._transforms:
            logger.warning("[%s] Unknown transform type: %s, skipping",
                           table_name, transform_type)
            return dataframe
        if column not in dataframe.columns:
            logger.warning("[%s] Column %s not found, skipping transform",
                           table_name, column)
            return dataframe
        try:
            logger.info("[%s] Applying transform '%s': type=%s, column=%s",
                        table_name, transform_id, transform_type, column)
            return self._transforms[transform_type](dataframe, column, params)
        except Exception as exc:
            logger.error("[%s] Transform %s failed: %s", table_name, transform_type, exc)
            return dataframe

    def _trim_transform(self, dataframe: DataFrame, column: str,
                        params: Dict[str, Any]) -> DataFrame:
        return dataframe.withColumn(column, self._trim(col(column)))

    def _lower_transform(self, dataframe: DataFrame, column: str,
                         params: Dict[str, Any]) -> DataFrame:
        return dataframe.withColumn(column, self._lower(col(column)))

    def _upper_transform(self, dataframe: DataFrame, column: str,
                         params: Dict[str, Any]) -> DataFrame:
        return dataframe.withColumn(column, self._upper(col(column)))

    def _round_transform(self, dataframe: DataFrame, column: str,
                         params: Dict[str, Any]) -> DataFrame:
        decimals = params.get("decimals", 2)
        return dataframe.withColumn(column, self._round(col(column), decimals))

    def _mask_pii_transform(self, dataframe: DataFrame, column: str,
                            params: Dict[str, Any]) -> DataFrame:
        visible = params.get("visible_chars", 4)
        return dataframe.withColumn(
            column,
            self._concat(self._substring(col(column), 1, visible), lit("****"))
        )


# Global registries
DQ_REGISTRY = None
TRANSFORM_REGISTRY = None


def get_registries():
    """Get or create global registries."""
    global DQ_REGISTRY, TRANSFORM_REGISTRY
    if DQ_REGISTRY is None:
        DQ_REGISTRY = DQRegistry()
    if TRANSFORM_REGISTRY is None:
        TRANSFORM_REGISTRY = TransformRegistry()
    return DQ_REGISTRY, TRANSFORM_REGISTRY



# ============================================================================
# Spark and Config
# ============================================================================
def get_spark() -> SparkSession:
    """Get or create Spark session with Delta Lake."""
    return SparkSession.builder \
        .appName("StreamingETL-DQ-Transform-SCD2") \
        .config("spark.sql.extensions",
                "io.delta.sql.DeltaSparkSessionExtension") \
        .config("spark.sql.catalog.spark_catalog",
                "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
        .getOrCreate()


def load_config_from_s3(s3_path: str) -> Dict[str, Any]:
    """Load YAML config from S3 and resolve placeholders."""
    logger.info("Loading config from: %s", s3_path)
    s3 = boto3.client("s3")
    sts = boto3.client("sts")
    
    path = s3_path.replace("s3://", "")
    bucket = path.split("/")[0]
    key = "/".join(path.split("/")[1:])
    
    # Get AWS account ID
    account_id = sts.get_caller_identity()["Account"]
    
    # Derive stack name from bucket name (format: <stack>-assets-<account>)
    stack_name = bucket.replace(f"-assets-{account_id}", "")
    
    response = s3.get_object(Bucket=bucket, Key=key)
    content = response["Body"].read().decode("utf-8")
    
    # Replace placeholders
    content = content.replace("${STACK_NAME}", stack_name)
    content = content.replace("${AWS_ACCOUNT_ID}", account_id)
    
    logger.info("Resolved config placeholders: STACK_NAME=%s, AWS_ACCOUNT_ID=%s", 
                stack_name, account_id)
    
    config = yaml.safe_load(content)
    
    # Resolve Kafka bootstrap servers from MSK
    if config.get("kafka", {}).get("bootstrap_servers") == "PLACEHOLDER_BOOTSTRAP_SERVERS":
        bootstrap = get_msk_bootstrap_servers(stack_name)
        config["kafka"]["bootstrap_servers"] = bootstrap
        logger.info("Resolved MSK bootstrap servers: %s", bootstrap)
    
    return config


def get_msk_bootstrap_servers(stack_name: str) -> str:
    """Get MSK bootstrap servers from the cluster."""
    kafka_client = boto3.client("kafka", region_name="us-east-1")
    
    # List clusters and find the one matching our stack
    clusters = kafka_client.list_clusters()
    for cluster in clusters.get("ClusterInfoList", []):
        if stack_name in cluster.get("ClusterName", ""):
            cluster_arn = cluster["ClusterArn"]
            response = kafka_client.get_bootstrap_brokers(ClusterArn=cluster_arn)
            bootstrap = response.get("BootstrapBrokerStringSaslScram", "")
            if bootstrap:
                return bootstrap
    
    raise Exception(f"Could not find MSK cluster for stack: {stack_name}")


def get_kafka_password(config: Dict) -> str:
    """Get Kafka password from Secrets Manager."""
    client = boto3.client("secretsmanager", region_name="us-east-1")
    # Secret name follows pattern: AmazonMSK_<env>_scram
    # Try to get from config's bootstrap servers to determine env name
    bootstrap = config.get("kafka", {}).get("bootstrap_servers", "")
    
    # List secrets and find the MSK one
    try:
        # Try common patterns
        secret_patterns = [
            "AmazonMSK_dq-etl-v7_scram",
            "AmazonMSK_etl-streaming_scram",
            "AmazonMSK_etl_scram_secret",
        ]
        
        for pattern in secret_patterns:
            try:
                response = client.get_secret_value(SecretId=pattern)
                secret = json.loads(response["SecretString"])
                logger.info("Found Kafka secret: %s", pattern)
                return secret.get("password", "")
            except client.exceptions.ResourceNotFoundException:
                continue
        
        # Fallback: list all secrets and find MSK one
        paginator = client.get_paginator('list_secrets')
        for page in paginator.paginate():
            for secret_entry in page.get('SecretList', []):
                if 'AmazonMSK_' in secret_entry['Name'] and '_scram' in secret_entry['Name']:
                    response = client.get_secret_value(SecretId=secret_entry['Name'])
                    secret = json.loads(response["SecretString"])
                    logger.info("Found Kafka secret via list: %s", secret_entry['Name'])
                    return secret.get("password", "")
        
        raise Exception("No MSK SCRAM secret found")
    except Exception as e:
        logger.error("Failed to get Kafka password: %s", e)
        raise


# ============================================================================
# Schema Building
# ============================================================================
def build_spark_type(type_str: str):
    """Convert config type string to Spark type."""
    from pyspark.sql.types import DoubleType
    type_lower = type_str.lower()
    if type_lower in ("string", "varchar", "text"):
        return StringType()
    if type_lower in ("integer", "int"):
        return IntegerType()
    if type_lower in ("long", "bigint"):
        return LongType()
    if type_lower in ("double", "float", "real"):
        return DoubleType()
    if type_lower == "timestamp":
        return TimestampType()
    if type_lower == "date":
        return DateType()
    if type_lower == "boolean":
        return BooleanType()
    if type_lower.startswith("decimal"):
        match = re.match(r"decimal\((\d+),(\d+)\)", type_lower)
        if match:
            return DecimalType(int(match.group(1)), int(match.group(2)))
        return DecimalType(10, 2)
    return StringType()


def build_schema_from_config(schema_config: List[Dict]) -> StructType:
    """Build Spark StructType from config schema definition."""
    fields = []
    for field_def in schema_config:
        name = field_def["name"]
        spark_type = build_spark_type(field_def["type"])
        nullable = field_def.get("nullable", True)
        fields.append(StructField(name, spark_type, nullable))
    return StructType(fields)


def build_dms_envelope_schema(data_schema: StructType) -> StructType:
    """Build schema for DMS CDC envelope with nested data."""
    metadata_schema = StructType([
        StructField("timestamp", StringType(), True),
        StructField("record-type", StringType(), True),
        StructField("operation", StringType(), True),
        StructField("schema-name", StringType(), True),
        StructField("table-name", StringType(), True),
    ])
    return StructType([
        StructField("data", data_schema, True),
        StructField("metadata", metadata_schema, True),
    ])



# ============================================================================
# Kafka Options
# ============================================================================
def get_kafka_options(config: Dict, topic: str, password: str) -> Dict:
    """Build Kafka connection options."""
    kafka_config = config["kafka"]
    jaas = (
        "org.apache.kafka.common.security.scram.ScramLoginModule required "
        f'username="{kafka_config["sasl_username"]}" '
        f'password="{password}";'
    )
    return {
        "kafka.bootstrap.servers": kafka_config["bootstrap_servers"],
        "subscribe": topic,
        "kafka.security.protocol": kafka_config["security_protocol"],
        "kafka.sasl.mechanism": kafka_config["sasl_mechanism"],
        "kafka.sasl.jaas.config": jaas,
        "startingOffsets": "earliest",
        "failOnDataLoss": "false",
    }


# ============================================================================
# Data Processing Functions
# ============================================================================
def parse_dms_messages(batch_df: DataFrame, table_config: Dict,
                       table_name: str) -> DataFrame:
    """Parse DMS CDC messages and add operation column."""
    data_schema = build_schema_from_config(table_config["schema"])
    envelope_schema = build_dms_envelope_schema(data_schema)

    parsed_df = batch_df.select(
        from_json(col("value").cast("string"), envelope_schema).alias("parsed")
    ).select(
        col("parsed.data.*"),
        col("parsed.metadata.operation").alias("_dms_op"),
    )

    result_df = parsed_df.withColumn(
        "_operation",
        when(col("_dms_op") == "insert", "INSERT")
        .when(col("_dms_op") == "update", "UPDATE")
        .when(col("_dms_op") == "delete", "DELETE")
        .when(col("_dms_op") == "load", "INSERT")
        .otherwise("UNKNOWN")
    ).drop("_dms_op")

    # Filter out rows where all data columns are null (control messages)
    data_cols = [f["name"] for f in table_config["schema"]]
    not_null_cond = None
    for c in data_cols:
        if not_null_cond is None:
            not_null_cond = col(c).isNotNull()
        else:
            not_null_cond = not_null_cond | col(c).isNotNull()

    if not_null_cond is not None:
        result_df = result_df.filter(not_null_cond)

    return result_df


def apply_dq_rules(dataframe: DataFrame, table_config: Dict,
                   dq_registry: DQRegistry, table_name: str) -> tuple:
    """Apply DQ rules from config. Returns (valid_df, failed_df)."""
    dq_rules = table_config.get("dq_rules", [])
    if not dq_rules:
        logger.info("[%s] No DQ rules configured", table_name)
        return dataframe, dataframe.limit(0)

    logger.info("[%s] Applying %d DQ rules", table_name, len(dq_rules))

    valid_df = dataframe
    all_failed = None

    for rule in dq_rules:
        rule_id = rule.get("id", "unknown")
        rule_type = rule.get("type")
        column = rule.get("column")
        params = rule.get("params", {})
        severity = rule.get("severity", "error")

        if not rule_type or not column:
            continue

        passed, failed = dq_registry.apply(valid_df, rule_type, column, params,
                                           table_name, rule_id)

        if failed.count() > 0 and severity == "error":
            failed_with_meta = failed.withColumn("_failed_rule", lit(rule_id))
            if all_failed is None:
                all_failed = failed_with_meta
            else:
                all_failed = all_failed.unionByName(failed_with_meta, allowMissingColumns=True)
            valid_df = passed

    return valid_df, all_failed if all_failed else dataframe.limit(0)



def apply_transforms(dataframe: DataFrame, table_config: Dict,
                     transform_registry: TransformRegistry,
                     table_name: str) -> DataFrame:
    """Apply transforms from config in order."""
    transforms = table_config.get("transforms", [])
    if not transforms:
        logger.info("[%s] No transforms configured", table_name)
        return dataframe

    logger.info("[%s] Applying %d transforms", table_name, len(transforms))
    sorted_transforms = sorted(transforms, key=lambda x: x.get("order", 0))

    result = dataframe
    for tx in sorted_transforms:
        tx_id = tx.get("id", "unknown")
        tx_type = tx.get("type")
        column = tx.get("column")
        params = tx.get("params", {})

        if not tx_type or not column:
            continue

        result = transform_registry.apply(result, tx_type, column, params,
                                          table_name, tx_id)

    return result


def write_to_quarantine(failed_df: DataFrame, table_name: str,
                        quarantine_path: str) -> None:
    """Write failed records to quarantine bucket."""
    if failed_df.isEmpty():
        return

    path = f"{quarantine_path}{table_name}/"
    count = failed_df.count()
    failed_df.withColumn("_quarantined_at", current_timestamp()) \
        .write.mode("append").parquet(path)
    logger.info("[%s] QUARANTINED %d records to %s", table_name, count, path)


def add_scd2_columns(dataframe: DataFrame) -> DataFrame:
    """Add SCD Type 2 metadata columns."""
    now = current_timestamp()
    return dataframe \
        .withColumn("_effective_from", now) \
        .withColumn("_effective_to", lit(None).cast("timestamp")) \
        .withColumn("_is_current", lit(True)) \
        .withColumn("_ingested_at", now)


def deduplicate_by_pk(dataframe: DataFrame, primary_key: str) -> DataFrame:
    """Deduplicate DataFrame by primary key, keeping latest record."""
    from pyspark.sql.functions import row_number
    window = Window.partitionBy(primary_key).orderBy(col("_ingested_at").desc())
    return dataframe.withColumn("_rn", row_number().over(window)) \
        .filter(col("_rn") == 1) \
        .drop("_rn")



# ============================================================================
# Main Batch Processing
# ============================================================================
def process_batch(batch_df: DataFrame, batch_id: int, table_config: Dict,
                  config: Dict, spark: SparkSession, deequ_analyzer=None) -> None:
    """Process a micro-batch with DQ, transforms, and SCD Type 2."""
    from delta.tables import DeltaTable

    table_name = table_config["name"]
    delta_path = table_config["delta_path"]
    pk = table_config["primary_key"]
    quarantine_path = config["settings"].get("quarantine_path", "")

    logger.info("[%s] BATCH %d START", table_name, batch_id)

    if batch_df.isEmpty():
        logger.info("[%s] Empty batch, skipping", table_name)
        return

    dq_registry, transform_registry = get_registries()

    # Step 1: Parse DMS envelope
    parsed_df = parse_dms_messages(batch_df, table_config, table_name)
    if parsed_df.count() == 0:
        logger.info("[%s] No data records after parsing", table_name)
        return

    # Step 2: Run Deequ analyzers (if available)
    deequ_checks = table_config.get("deequ_checks", [])
    if deequ_analyzer and deequ_checks:
        metrics_df = deequ_analyzer.analyze_batch(parsed_df, table_name, batch_id, deequ_checks)
        if not metrics_df.isEmpty():
            deequ_analyzer.write_metrics(metrics_df)

    # Step 3: Apply DQ rules
    valid_df, failed_df = apply_dq_rules(parsed_df, table_config, dq_registry, table_name)
    
    if quarantine_path and not failed_df.isEmpty():
        write_to_quarantine(failed_df, table_name, quarantine_path)

    if valid_df.count() == 0:
        logger.info("[%s] No valid records after DQ", table_name)
        return

    # Step 4: Apply transforms
    transformed_df = apply_transforms(valid_df, table_config, transform_registry, table_name)

    # Step 5: Add SCD2 columns
    staged_df = add_scd2_columns(transformed_df)

    # Step 6: Deduplicate
    staged_df = deduplicate_by_pk(staged_df, pk)
    final_count = staged_df.count()

    if final_count == 0:
        return

    # Step 7: Write to Delta
    if not DeltaTable.isDeltaTable(spark, delta_path):
        inserts = staged_df.filter(col("_operation") == "INSERT")
        if inserts.count() > 0:
            logger.info("[%s] Creating Delta table at %s", table_name, delta_path)
            inserts.write.format("delta").mode("overwrite").save(delta_path)
        return

    # SCD2 MERGE
    delta_table = DeltaTable.forPath(spark, delta_path)
    merge_cond = f"target.{pk} = source.{pk} AND target._is_current = true"

    # Close old records for updates/deletes
    updates_deletes = staged_df.filter(col("_operation").isin("UPDATE", "DELETE"))
    if updates_deletes.count() > 0:
        delta_table.alias("target").merge(
            updates_deletes.alias("source"), merge_cond
        ).whenMatchedUpdate(set={
            "_is_current": lit(False),
            "_effective_to": col("source._effective_from"),
        }).execute()

    # Insert new records
    new_records = staged_df.filter(col("_operation").isin("INSERT", "UPDATE"))
    if new_records.count() > 0:
        new_records.write.format("delta").mode("append").save(delta_path)

    logger.info("[%s] BATCH %d COMPLETE - %d records", table_name, batch_id, final_count)



# ============================================================================
# Stream Management
# ============================================================================
def start_stream_for_table(spark: SparkSession, config: Dict,
                           table_config: Dict, kafka_password: str,
                           deequ_analyzer=None) -> Any:
    """Start a streaming query for a single table."""
    table_name = table_config["name"]
    topic = table_config["topic"]
    checkpoint = config["settings"]["checkpoint_location"] + table_name + "/"

    logger.info("[%s] Starting stream from topic: %s", table_name, topic)

    kafka_options = get_kafka_options(config, topic, kafka_password)

    stream_df = spark.readStream \
        .format("kafka") \
        .options(**kafka_options) \
        .load()

    def batch_processor(df, bid):
        process_batch(df, bid, table_config, config, spark, deequ_analyzer)

    query = stream_df.writeStream \
        .foreachBatch(batch_processor) \
        .option("checkpointLocation", checkpoint) \
        .trigger(processingTime=config["settings"]["trigger_interval"]) \
        .queryName("stream_" + table_name) \
        .start()

    logger.info("[%s] Stream started, query ID: %s", table_name, query.id)
    return query


def main():
    """Main entry point."""
    logger.info("GLUE STREAMING ETL - DQ + TRANSFORMS + SCD2")

    config_path = "s3://streaming-etl-assets/config/tables.yaml"
    try:
        from awsglue.utils import getResolvedOptions
        args = getResolvedOptions(sys.argv, ["JOB_NAME", "CONFIG_PATH"])
        config_path = args["CONFIG_PATH"]
    except Exception:
        logger.warning("Glue args not available, using default config path")

    logger.info("Config path: %s", config_path)

    spark = get_spark()
    config = load_config_from_s3(config_path)
    kafka_password = get_kafka_password(config)

    get_registries()

    # Initialize Deequ analyzer (REQUIRED for enterprise DQ metrics)
    delta_bucket = config["settings"].get("delta_bucket", "streaming-etl-delta")
    deequ_state_path = f"s3://{delta_bucket}/deequ-state/"
    deequ_metrics_path = f"s3://{delta_bucket}/dq_metrics/"
    deequ_analyzer = create_deequ_analyzer(spark, deequ_state_path, deequ_metrics_path)
    logger.info("Deequ analyzer initialized - state: %s, metrics: %s",
                deequ_state_path, deequ_metrics_path)

    tables = config.get("tables", [])
    streaming_tables = [t for t in tables if not t.get("internal", False)]
    logger.info("Found %d tables to process", len(streaming_tables))

    queries = []
    for table_config in streaming_tables:
        try:
            query = start_stream_for_table(spark, config, table_config, kafka_password,
                                           deequ_analyzer)
            queries.append(query)
        except Exception as exc:
            logger.error("Failed to start stream for %s: %s", table_config["name"], exc)

    logger.info("Started %d streaming queries", len(queries))
    spark.streams.awaitAnyTermination()


if __name__ == "__main__":
    main()
